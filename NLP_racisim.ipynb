{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef3be5-ad8c-4bf3-b925-9161f9426746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'DiscriminatoryText (1)/DiscriminatoryText.csv'\n",
    "df = pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove non-alphabet characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['prompt'] = df['prompt'].apply(preprocess_text)\n",
    "\n",
    "# Handle negation\n",
    "def handle_negation(text):\n",
    "    # List of negation words\n",
    "    negation_words = [\"not\", \"no\", \"never\", \"none\"]\n",
    "    negation_dict = {\n",
    "        \"good\": \"bad\", \"happy\": \"sad\", \"like\": \"dislike\", \"love\": \"hate\",\n",
    "        \"bad\": \"good\", \"sad\": \"happy\", \"dislike\": \"like\", \"hate\": \"love\"\n",
    "    }\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    skip = False\n",
    "    for i in range(len(words)):\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        if words[i] in negation_words and i+1 < len(words):\n",
    "            word = negation_dict.get(words[i+1], words[i+1])\n",
    "            new_words.append(word)\n",
    "            skip = True\n",
    "        else:\n",
    "            new_words.append(words[i])\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "df['prompt'] = df['prompt'].apply(handle_negation)\n",
    "\n",
    "# Preprocessing and dataset preparation\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Convert examples to InputExamples\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    for idx, example in examples.iterrows():\n",
    "        text, label = example['prompt'], example['prompt_label']\n",
    "        inputs = tokenizer.encode_plus(text, max_length=max_length, truncation=True, padding='max_length', add_special_tokens=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        attention_masks.append(inputs['attention_mask'])\n",
    "        labels.append(label)\n",
    "    return tf.data.Dataset.from_tensor_slices(({'input_ids': input_ids, 'attention_mask': attention_masks}, labels))\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(train, tokenizer)\n",
    "val_data = convert_examples_to_tf_dataset(val, tokenizer)\n",
    "test_data = convert_examples_to_tf_dataset(test, tokenizer)\n",
    "\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "val_data = val_data.batch(32)\n",
    "test_data = test_data.batch(32)\n",
    "\n",
    "# Load BERT model for sequence classification\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-8)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66533cc6-b5f3-48ea-999b-9d542714a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8faaeb-2d0e-40d5-933f-7fb771647f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with callbacks\n",
    "history = model.fit(train_data, epochs=3, validation_data=val_data, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_data)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# Get classification report\n",
    "y_true = np.concatenate([y for x, y in test_data], axis=0)\n",
    "y_pred = np.argmax(model.predict(test_data)[0], axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['Non-Racist', 'Racist']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81841ab-18bb-4203-870e-737a54b3e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_data_list)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3787f25-e342-4e08-a4a4-0b97be353dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_subset = test_data.take(200)\n",
    "\n",
    "# Evaluate the model with the subset of test data\n",
    "loss, accuracy = model.evaluate(test_data_subset)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744d2cd-c607-4b8c-8e56-2f0422fa3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, model, tokenizer, max_length=128):\n",
    "    # Preprocess text\n",
    "    text = preprocess_text(text)\n",
    "    text = handle_negation(text)\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer.encode_plus(text, max_length=max_length, truncation=True, padding='max_length', add_special_tokens=True)\n",
    "    input_ids = np.array([inputs['input_ids']])\n",
    "    attention_mask = np.array([inputs['attention_mask']])\n",
    "    # Predict\n",
    "    predictions = model.predict({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "    label_id = np.argmax(predictions[0], axis=1).flatten()\n",
    "    return label_id[0]\n",
    "\n",
    "# Example usage\n",
    "text_to_classify = \"Indian food are always\"\n",
    "prediction = classify_text(text_to_classify, model, tokenizer)\n",
    "if prediction == 0:\n",
    "    print(\"Non-Racist\")\n",
    "else:\n",
    "    print(\"Racist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3be77e-ebfb-4e40-b1f9-5015381b9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./saved_model2/tokenizer')\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained('./saved_model2/model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
